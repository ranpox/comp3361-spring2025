{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfwAdXU4urVp"
      },
      "source": [
        "## Section 1: Basic Decoding Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUc1HaH0J1M4"
      },
      "outputs": [],
      "source": [
        "!uv pip install transformers datasets evaluate markdownify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7WLNZn_9d7n"
      },
      "outputs": [],
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJrHDSAIJ7ku"
      },
      "outputs": [],
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H4QKWbtJ9MW"
      },
      "outputs": [],
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUuU65RsJ_GI"
      },
      "outputs": [],
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxU5PdFtKBJO"
      },
      "outputs": [],
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zXVylNcKFOJ"
      },
      "source": [
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Vanilla sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO8x8XJaKEwF"
      },
      "outputs": [],
      "source": [
        "\"\"\"decode main wrapper function\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {'attention_mask': attention_mask, \"use_cache\": False}\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    cache_position = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
        "    model_kwargs[\"cache_position\"] = cache_position\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c17DrQISKNzI"
      },
      "outputs": [],
      "source": [
        "\"\"\"debug helper code\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFGrlRSXYn"
      },
      "source": [
        "### 1.1 Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH0wBhy2SZa-"
      },
      "outputs": [],
      "source": [
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute `next_tokens` from `next_token_logits`.\n",
        "  # Hint: use torch.argmax()\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKL_31VJNw1"
      },
      "source": [
        "### 1.2 Vanilla Sampling and Temperature Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjrTLKj2JR5b"
      },
      "outputs": [],
      "source": [
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs =\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  # Hint: use torch.multinomial()\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Su03uzJb_Z"
      },
      "outputs": [],
      "source": [
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: compute the probabilities `probs` from the logits, with temperature applied.\n",
        "  probs =\n",
        "\n",
        "  # TODO: compute `next_tokens` from `probs`.\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqWUJiFK43O"
      },
      "source": [
        "### 1.3 Top-k Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjXyfL4GK3mK"
      },
      "outputs": [],
      "source": [
        "def topk(next_token_logits, k):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - k: int\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Keep only top-k tokens with highest probabilities.\n",
        "  # Hint: use torch.topk()\n",
        "  topk_logits, topk_indices =\n",
        "\n",
        "  # Create a mask to zero out all logits not in top-k\n",
        "  indices_to_remove = next_token_logits < topk_logits[:, -1].unsqueeze(1)\n",
        "\n",
        "  # Mask the logits\n",
        "  next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # TODO: Sample from the masked logits\n",
        "  probs =\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topk', k=20)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSjMWNFEy_cC"
      },
      "source": [
        "### 1.4 Top-p Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq1ZwVVxzApa"
      },
      "outputs": [],
      "source": [
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  sorted_logits, sorted_indices =\n",
        "  sorted_probs =\n",
        "  cum_probs =\n",
        "\n",
        "  # Create a mask to zero out all logits not in top-p\n",
        "  sorted_indices_to_remove = cum_probs > p\n",
        "  sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "  sorted_indices_to_remove[:, 0] = 0\n",
        "  # Restore mask to original indices\n",
        "  indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "  # Mask the logits\n",
        "  next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # TODO: Sample from the masked logits\n",
        "  probs =\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGfhvyy0Ka8i"
      },
      "source": [
        "### 1.5: Evaluation\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGyyCSgOKbgw"
      },
      "outputs": [],
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUc_IncHKkRF"
      },
      "source": [
        "### Discussion\n",
        "- Q1.1: In greedy decoding, what do you observe when generating 10 times from the test prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoDRS10iKzzc"
      },
      "source": [
        "- Q1.2: In vanilla sampling, what do you observe when generating 10 times from the test prompt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPXsI0YK1Db"
      },
      "source": [
        "- Q1.3: In temperature sampling, play around with the value of temperature $t$. Which value of $t$ makes it equivalent to greedy decoding? Which value of $t$ makes it equivalent to vanilla sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE16fo91K2VM"
      },
      "source": [
        "- Q1.4: In top-$p$ sampling, play around with the value of $p$. Which value of $p$ makes it equivalent to greedy decoding? Which value of $p$ makes it equivalent to vanilla sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wMu-w_FK3z5"
      },
      "source": [
        "- Q1.5: In top-k sampling, play around with the value of k. Which value of k makes it equivalent to greedy decoding? Which value of k makes it equivalent to vanilla sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ASl7jxoDR-"
      },
      "source": [
        "- Q1.6: Report the evaluation metrics (perplexity, fluency, diversity) of all 5 decoding methods. Which methods have the best and worst perplexity? Fluency? Diversity?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njf8q19ZJ4cJ"
      },
      "source": [
        "## Section 2: Applying Large Language Models to Few Shot Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMVRgAIkj0Ve"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q vllm # restart runtime session after install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQvzOL51kwNO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_HOME\"] = \"/content/.cache/huggingface\" # set the cache directory to personal disk to avoid frequent downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX7-W5OEko_Y"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "model_id = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
        "llm = LLM(model=model_id, enforce_eager=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pxe1n_slCJx"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class VLLMClient:\n",
        "    def __init__(self, model_id, **kwargs):\n",
        "      self.model_id = model_id\n",
        "\n",
        "    def __call__(self, prompt: str, **kwargs):\n",
        "      response = llm.generate(\n",
        "            prompts=prompt,\n",
        "            sampling_params=SamplingParams(\n",
        "                temperature=kwargs.get(\"temperature\", 0.2),\n",
        "                max_tokens=kwargs.get(\"max_tokens\", 256),\n",
        "            )\n",
        "      )\n",
        "      return response[0].outputs[0].text\n",
        "model = VLLMClient(model_id)\n",
        "model(\"San Francisco is a\", max_tokens=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9k51qESwRzp"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "GSM_EXAMPLARS = [\n",
        "    {\n",
        "        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"cot_answer\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\",\n",
        "        \"short_answer\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "        \"cot_answer\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\",\n",
        "        \"short_answer\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
        "        \"cot_answer\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\",\n",
        "        \"short_answer\": \"39\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n",
        "        \"cot_answer\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\",\n",
        "        \"short_answer\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n",
        "        \"cot_answer\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\",\n",
        "        \"short_answer\": \"9\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n",
        "        \"cot_answer\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\",\n",
        "        \"short_answer\": \"29\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n",
        "        \"cot_answer\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\",\n",
        "        \"short_answer\": \"33\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n",
        "        \"cot_answer\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\",\n",
        "        \"short_answer\": \"8\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyR0w4d62rf7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "!wget -q -O data/gsm8k.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/gsm8k.jsonl\n",
        "!wget -q -O data/simpleqa.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/simpleqa.jsonl\n",
        "!wget -q -O data/math.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/math.jsonl\n",
        "!wget -q -O data/gaia.jsonl https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/data/gaia.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTh2lX8V2l8j"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "import json\n",
        "\n",
        "def load_eval_data(task):\n",
        "    with open(f\"data/{task}.jsonl\", \"r\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "tasks = [\"gsm8k\", \"simpleqa\", \"math\", \"gaia\"]\n",
        "\n",
        "for task in tasks:\n",
        "    print(f\"Example of {task} dataset:\")\n",
        "    print(json.dumps(load_eval_data(task)[0], indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wypUrNwJ4s0W"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "APPEND_ANSWER_LOCK = threading.Lock()\n",
        "\n",
        "def append_answer(entry: dict, jsonl_file: str) -> None:\n",
        "    jsonl_file = Path(jsonl_file)\n",
        "    jsonl_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with APPEND_ANSWER_LOCK, open(jsonl_file, \"a\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(json.dumps(entry) + \"\\n\")\n",
        "    assert os.path.exists(jsonl_file), \"File not found!\"\n",
        "\n",
        "def answer_single_question(example, agent, answers_file, action_type):\n",
        "    augmented_question = example[\"question\"]\n",
        "    if example[\"source\"] == \"SimpleQA\":\n",
        "        augmented_question += \" Answer with only the final number.\"\n",
        "    if example[\"source\"] == \"MATH\":\n",
        "        augmented_question += \" Write code, not latex.\"\n",
        "\n",
        "    start_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    answer = str(agent.run(augmented_question))\n",
        "\n",
        "    end_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    annotated_example = {\n",
        "        \"model_id\": model.model_id,\n",
        "        \"agent_action_type\": action_type,\n",
        "        \"question\": augmented_question,\n",
        "        \"original_question\": example[\"question\"],\n",
        "        \"answer\": answer,\n",
        "        \"true_answer\": example[\"true_answer\"],\n",
        "        \"source\": example[\"source\"],\n",
        "        \"start_time\": start_time,\n",
        "        \"end_time\": end_time,\n",
        "    }\n",
        "    append_answer(annotated_example, answers_file)\n",
        "\n",
        "def answer_questions(\n",
        "    task,\n",
        "    agent,\n",
        "    action_type: str = \"vanilla\",\n",
        "    answers_file: str = None,\n",
        "    parallel_workers: int = 4,\n",
        "):\n",
        "    eval_data = load_eval_data(task)\n",
        "\n",
        "    print(f\"Starting processing and writing output to '{answers_file}'\")\n",
        "\n",
        "    answered_questions = []\n",
        "    if os.path.exists(answers_file):\n",
        "        with open(answers_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                answered_questions.append(json.loads(line)[\"original_question\"])\n",
        "\n",
        "    examples_todo = [example for example in eval_data if example[\"question\"] not in answered_questions]\n",
        "\n",
        "\n",
        "    for i, example in enumerate(tqdm(examples_todo)):\n",
        "        answer_single_question(example, agent, answers_file, action_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxayWofTGe0b"
      },
      "outputs": [],
      "source": [
        "!wget -q -O eval_utils.py https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/evaluate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYPInNG85oQr"
      },
      "outputs": [],
      "source": [
        "from eval_utils import score_answers\n",
        "class FewShotReasoner:\n",
        "    def __init__(self, model, n_shots):\n",
        "        self.model = model\n",
        "        self.n_shots = n_shots\n",
        "\n",
        "    def run(self, task):\n",
        "        # TODO\n",
        "\n",
        "    def build_input(self, task):\n",
        "        # TODO\n",
        "\n",
        "\n",
        "def run_gsm8k_fewshot_reasoner(task=\"gsm8k\", model_id=model_id, action_type=\"vanilla\"):\n",
        "    reasoner_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n",
        "    chat_agent = FewShotReasoner(model, 8)\n",
        "    answer_questions(task, chat_agent, action_type, reasoner_answers_file)\n",
        "    df = score_answers([reasoner_answers_file])\n",
        "    print(df)\n",
        "\n",
        "run_gsm8k_fewshot_reasoner()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWT7nVRSD-Ao"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class FewShotCoTReasoner:\n",
        "    def __init__(self, model, n_shots):\n",
        "        self.model = model\n",
        "        self.n_shots = n_shots\n",
        "\n",
        "    def run(self, task):\n",
        "        # TODO\n",
        "\n",
        "    def build_input(self, task):\n",
        "        # TODO\n",
        "\n",
        "\n",
        "def run_gsm8k_fewshot_cot_reasoner(task=\"gsm8k\", model_id=model_id, action_type=\"vanilla\"):\n",
        "    reasoner_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}__cot.jsonl\"\n",
        "    chat_agent = FewShotCoTReasoner(model, 8)\n",
        "    answer_questions(task, chat_agent, action_type, reasoner_answers_file)\n",
        "    df = score_answers([reasoner_answers_file])\n",
        "    print(df)\n",
        "\n",
        "run_gsm8k_fewshot_cot_reasoner()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIY-UV7dJbfm"
      },
      "source": [
        "## Section 3:Building and Evaluating LLM Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTRzKmQrHhSw"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "class ChatAgent:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model.model_id)\n",
        "\n",
        "    def run(self, task):\n",
        "        prompt = self.build_input(task)\n",
        "        return self.model(prompt=prompt)\n",
        "\n",
        "    def build_input(self, task):\n",
        "        messages = [{\"role\": \"user\", \"content\": task}]\n",
        "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "action_type=\"vanilla\"\n",
        "chat_agent_answers_files = {}\n",
        "for task in [\"simpleqa\", \"math\", \"gaia\"]:\n",
        "    chat_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n",
        "    chat_agent = ChatAgent(model)\n",
        "    answer_questions(task, chat_agent, action_type, chat_agent_answers_file, parallel_workers=8)\n",
        "    chat_agent_answers_files[task] = chat_agent_answers_file\n",
        "score_answers(chat_agent_answers_files.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeTw_pg6OAKL"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from io import StringIO\n",
        "import sys\n",
        "import inspect\n",
        "from typing import Any, Dict, Union, Optional, Tuple, List\n",
        "\n",
        "class Tool:\n",
        "    name: str\n",
        "    description: str\n",
        "    inputs: Dict[str, Dict[str, Union[str, type, bool]]]\n",
        "    output_type: str\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.is_initialized = False\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return NotImplementedError(\"Write this method in your subclass of `Tool`.\")\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        outputs = self.forward(*args, **kwargs)\n",
        "        return outputs\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": self.name,\n",
        "                \"description\": self.description,\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": self.inputs,\n",
        "                    \"required\": list(self.inputs.keys()),\n",
        "                },\n",
        "            },\n",
        "            \"strict\": True\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEHWM_jPOn6U"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "class FinalAnswerTool(Tool):\n",
        "    name = \"final_answer\"\n",
        "    description = \"Provides a final answer to the given problem. Make sure to use this tool to provide the final answer at the end of the task.\"\n",
        "    inputs = {\"answer\": {\"type\": \"any\", \"description\": \"The final answer to the problem\"}}\n",
        "    output_type = \"any\"\n",
        "\n",
        "    def forward(self, answer: Any) -> Any:\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdZc8TUOs0N"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "final_answer_tool = FinalAnswerTool()\n",
        "final_answer_tool(\"Hello, world!\")\n",
        "\n",
        "tools = [\n",
        "    final_answer_tool.to_dict()\n",
        "]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What year was the municipality of Ramiriqu\\u00ed, Boyac\\u00e1, Colombia, founded?  Answer with only the final number.\"}\n",
        "]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model.model_id)\n",
        "print(\"=\"*10 + \"Input Prompt\" + \"=\"*10 + \"\\n\")\n",
        "input_prompt = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=True)\n",
        "print(\"=\"*10 + \"Response\" + \"=\"*10 + \"\\n\")\n",
        "response = model(tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=True))\n",
        "print(\"\\n\\n\" + response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEcoTB-pQkxg"
      },
      "outputs": [],
      "source": [
        "class GoogleSearchTool(Tool):\n",
        "    name = \"web_search\"\n",
        "    description = \"\"\"Performs a google web search for your query then returns a string of the top search results.\"\"\"\n",
        "    inputs = {\n",
        "        \"query\": {\"type\": \"string\", \"description\": \"The search query to perform.\"},\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, provider: str = \"serper\"):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        import requests\n",
        "\n",
        "        # Register Google Search Api through https://serper.dev/\n",
        "        # Use the organic key from search results to build this tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxstLV5AQuIf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPER_API_KEY\"] = \"APIKEY\" # remember to remove your key for submission\n",
        "google_search_tool = GoogleSearchTool()\n",
        "google_search_tool(\"Space Exploration Technologies Corp.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RVA8fEoZrmv"
      },
      "outputs": [],
      "source": [
        "!uv pip install markdownify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_dtjlhJQo4q"
      },
      "outputs": [],
      "source": [
        "class VisitWebpageTool(Tool):\n",
        "    name = \"visit_webpage\"\n",
        "    description = (\n",
        "        \"Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages.\"\n",
        "    )\n",
        "    inputs = {\n",
        "        \"url\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The url of the webpage to visit.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, max_output_length: int = 10000):\n",
        "        super().__init__()\n",
        "        self.max_output_length = max_output_length\n",
        "\n",
        "    def forward(self, url: str) -> str:\n",
        "        try:\n",
        "            import re\n",
        "\n",
        "            import requests\n",
        "            from markdownify import markdownify\n",
        "            from requests.exceptions import RequestException\n",
        "        except ImportError as e:\n",
        "            raise ImportError(\n",
        "                \"You must install packages `markdownify` and `requests` to run this tool: for instance run `pip install markdownify requests`.\"\n",
        "            ) from e\n",
        "        try:\n",
        "            # TODO\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            return \"The request timed out. Please try again later or check the URL.\"\n",
        "        except RequestException as e:\n",
        "            return f\"Error fetching the webpage: {str(e)}\"\n",
        "        except Exception as e:\n",
        "            return f\"An unexpected error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMj8Ftq1Rn99"
      },
      "outputs": [],
      "source": [
        "visit_page_tool = VisitWebpageTool()\n",
        "visit_page_tool(\"https://en.wikipedia.org/wiki/SpaceX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb5YdJJHSfSU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Callable, Optional\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\n",
        "To do so, you have been given access to some tools.\n",
        "\n",
        "Here are the rules you should always follow to solve your task:\n",
        "1. ALWAYS provide a tool call, else you will fail.\n",
        "2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.\n",
        "3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\n",
        "If no tool call is needed, use final_answer tool to return your answer.\n",
        "4. Never re-do a tool call that you previously did with the exact same parameters.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ToolCallAgent:\n",
        "    \"\"\"\n",
        "    A minimal agent that uses a language model to interact with tools\n",
        "    based on the ToolCallingAgent pattern.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Callable,\n",
        "        tools: List[Tool],\n",
        "        max_steps: int = 5,\n",
        "        final_answer_tool_name: str = \"final_answer\",\n",
        "        print_log: bool = True\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tools_definitions = [tool.to_dict() for tool in tools]\n",
        "        self.tool_executors = {tool.name: tool for tool in tools}\n",
        "        self.max_steps = max_steps\n",
        "        self.final_answer_tool_name = final_answer_tool_name\n",
        "\n",
        "        self.log_func = lambda x: print(x) if print_log else lambda _: None\n",
        "\n",
        "        # Validate that the final answer tool has an executor\n",
        "        if self.final_answer_tool_name not in self.tool_executors:\n",
        "            raise ValueError(f\"Executor for final answer tool '{self.final_answer_tool_name}' not found in tool_executors.\")\n",
        "\n",
        "    def _execute_tool(self, tool_name: str, parsed_args: Any) -> str:\n",
        "        executor = self.tool_executors[tool_name]\n",
        "        try:\n",
        "            self.log_func(f\"Executing tool '{tool_name}'...\")\n",
        "\n",
        "            # TODO\n",
        "\n",
        "            self.log_func(f\"Tool '{tool_name}' executed.\")\n",
        "            # The final_answer tool itself returns the value, we don't stringify it here yet\n",
        "            return tool_result\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing tool '{tool_name}' with args {parsed_args}: {e}\")\n",
        "            # Return the error message as the observation\n",
        "            return f\"Error during execution of tool '{tool_name}': {e}\"\n",
        "\n",
        "    def _parse_tool_arguments(self, content: str):\n",
        "        \"\"\"Try parse the tool calls.\"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def run(self, task: str) -> Any:\n",
        "        \"\"\"\n",
        "        Runs the agent loop starting with the initial messages.\n",
        "\n",
        "        Args:\n",
        "            task: The initial user prompt.\n",
        "\n",
        "        Returns:\n",
        "            The final answer extracted from the final_answer tool call,\n",
        "            or None if max_steps is reached or an error occurs.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": task}\n",
        "        ]\n",
        "\n",
        "        self.log_func(f\"Starting agent run with max_steps={self.max_steps}\")\n",
        "        self.log_func(\"Initial messages:\")\n",
        "        self.log_func(json.dumps(messages, indent=2))\n",
        "        self.log_func(\"-\" * 30)\n",
        "\n",
        "        for step in range(self.max_steps):\n",
        "            self.log_func(f\"--- Step {step + 1} ---\")\n",
        "            self.log_func(json.dumps(messages, indent=2))\n",
        "\n",
        "            # TODO\n",
        "            # For guidance on handling and executing function calls, please refer to [this documentation](https://docs.sglang.ai/backend/function_calling.html).\n",
        "\n",
        "            self.log_func(\"-\" * 30) # Separator for next step\n",
        "\n",
        "        self.log_func(\"Maximum steps reached without a final answer.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwKOxzveSCDD"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def run_simpleqa_tool_calling_agent(task=\"simpleqa\", model_id=model_id, action_type=\"tool_calling\"):\n",
        "    search_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n",
        "    search_agent = ToolCallAgent(\n",
        "        model=model,\n",
        "        tools=[GoogleSearchTool(), VisitWebpageTool(), FinalAnswerTool()],\n",
        "        max_steps=10\n",
        "    )\n",
        "    answer_questions(task, search_agent, action_type, search_agent_answers_file)\n",
        "    df = score_answers([search_agent_answers_file])\n",
        "    print(df)\n",
        "\n",
        "run_simpleqa_tool_calling_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oco5MCGVOJt_"
      },
      "outputs": [],
      "source": [
        "!wget -q -O local_python_executor.py https://raw.githubusercontent.com/ranpox/comp3361-spring2025/refs/heads/main/assignments/A3/local_python_executor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBUuNgOIWoA7"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from local_python_executor import (\n",
        "    BASE_BUILTIN_MODULES,\n",
        "    BASE_PYTHON_TOOLS,\n",
        "    evaluate_python_code,\n",
        ")\n",
        "\n",
        "\n",
        "class PythonInterpreterTool(Tool):\n",
        "    name = \"python_interpreter\"\n",
        "    description = \"This is a tool for evaluating python code. It can be used to perform calculations. To generate valid Python code, first draft your code in Markdown using ```python```, then write it as a one-line string in the `code` field\"\n",
        "    inputs = {\n",
        "        \"code\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The python code to run in interpreter\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, *args, authorized_imports=None, **kwargs):\n",
        "        if authorized_imports is None:\n",
        "            self.authorized_imports = list(set(BASE_BUILTIN_MODULES))\n",
        "        else:\n",
        "            self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(authorized_imports))\n",
        "        self.inputs = {\n",
        "            \"code\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": (\n",
        "                    \"The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, \"\n",
        "                    f\"else you will get an error. This code can only import the following python libraries: {self.authorized_imports}.\"\n",
        "                ),\n",
        "            }\n",
        "        }\n",
        "        self.base_python_tools = BASE_PYTHON_TOOLS\n",
        "        self.python_evaluator = evaluate_python_code\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, code: str) -> str:\n",
        "        state = {}\n",
        "        output = str(\n",
        "            self.python_evaluator(\n",
        "                code,\n",
        "                state=state,\n",
        "                static_tools=self.base_python_tools,\n",
        "                authorized_imports=self.authorized_imports,\n",
        "            )  # The second element is boolean is_final_answer\n",
        "        )\n",
        "        return f\"Stdout:\\n{str(state['_print_outputs'])}\\nOutput: {output}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofg3MzBzWZ0x"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from eval_utils import score_answers\n",
        "\n",
        "def run_math_tool_calling_agent(task=\"math\", model_id=model_id, action_type=\"tool_calling\"):\n",
        "    math_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n",
        "    math_agent = ToolCallAgent(\n",
        "        model=model,\n",
        "        tools=[PythonInterpreterTool(authorized_imports=[\"numpy\", \"sympy\"]), FinalAnswerTool()],\n",
        "        max_steps=10\n",
        "    )\n",
        "    answer_questions(task, math_agent, action_type, math_agent_answers_file)\n",
        "    df = score_answers([math_agent_answers_file])\n",
        "    print(df)\n",
        "\n",
        "run_math_tool_calling_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkg8Br0icJMB"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from eval_utils import score_answers\n",
        "\n",
        "def run_gaia_tool_calling_agent(task=\"gaia\", model_id=model_id, action_type=\"tool_calling\"):\n",
        "    research_agent_answers_file = f\"output/{model_id.replace('/', '__')}__{action_type}__{task}.jsonl\"\n",
        "    research_agent = ToolCallAgent(\n",
        "        model=model,\n",
        "        tools=[GoogleSearchTool(), VisitWebpageTool(), PythonInterpreterTool(authorized_imports=[\"numpy\", \"sympy\"]), FinalAnswerTool()],\n",
        "        max_steps=10\n",
        "    )\n",
        "    answer_questions(task, research_agent, action_type, research_agent_answers_file)\n",
        "    df = score_answers([research_agent_answers_file])\n",
        "    print(df)\n",
        "\n",
        "run_gaia_tool_calling_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xWWEdTId0FU"
      },
      "source": [
        "| Task        | Action Type | Performance      |\n",
        "|-------------|-----|-----------------|\n",
        "| SimpleQA       | vanilla  |  |\n",
        "| SimpleQA       | tool calling  |  |\n",
        "| MATH       | vanilla  |  |\n",
        "| MATH       | tool calling  |  |\n",
        "| GAIA       | vanilla  |  |\n",
        "| GAIA       | tool calling  |  |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
